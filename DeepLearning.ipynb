{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://datamics.com/de/courses/\"><img src=../DATA/bg_datamics_top.png></a><em text-align:center>© Datamics</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Projekt - Lösung\n",
    "\n",
    "Wir werden nun mit unserem Tensorflow Projekt den Deep Learning Abschnitt abschließen. Dazu schauen wir uns die Effizienz von Deep Neural Nets an!\n",
    "\n",
    "Wir werden einen [Bank Authentifizierungs Datensatz](https://archive.ics.uci.edu/ml/datasets/banknote+authentication) aus dem USI Repository verwenden.\n",
    "\n",
    "Der Datensatz beinhaltet 5 Spalten:\n",
    "\n",
    "* variance of Wavelet Transformed image (continuous)\n",
    "* skewness of Wavelet Transformed image (continuous)\n",
    "* curtosis of Wavelet Transformed image (continuous)\n",
    "* entropy of image (continuous)\n",
    "* class (integer)\n",
    "\n",
    "Hier gibt \"class\" an, ob eine Banknote authentifiziert wurde oder nicht. \n",
    "\n",
    "Diese Art von Aufgabe ist perfekt für Neuronale Netze und Deep Learning! Folge den Anweisungen und löse die Aufgaben. Viel Spaß!\n",
    "\n",
    "## Die Daten Laden\n",
    "\n",
    "**Nutze Pandas, um die `bank_note_data.csv` Datei aus diesem verzeichnis zu öffnen.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://github.com/Soley02/DeepLearning/blob/main/Bank_Note_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Bank_Note_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schaue dir den head des DataFrames an.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Daten Analyse\n",
    "\n",
    "Wir erzeugen einige Visualisierungen für einen allgemeinen Überblick:\n",
    "\n",
    "**Importiere Seaborn und setzte matplotlib inline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erstelle ein `countplot` für die \"Classes\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Class',data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erstelle ein `pairplot` und setze die `hue` zu Class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data,hue='Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten Vorbereitung\n",
    "\n",
    "Wenn wir Neuronale Netze und Deep Learning basierte Systeme nutzen, dann empfiehlt es sich üblicherweise die Daten zu Standardisieren. Dieser Schritt ist für unseren konkreten Datensatz nicht nötig, wir möchten aber doch einmal gemeinsam durchgehen.\n",
    "\n",
    "### Standard Scaling\n",
    "\n",
    "**Importiere `StandardScaler` aus `sklearn.preprocessing`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erstelle ein StandardScaler() Objekt namens \"scaler\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitte scaler auf die Features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(data.drop('Class',axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nutze die `transform()` Methode, um die Features in eine skalierte Version umzuwandeln.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = scaler.fit_transform(data.drop('Class',axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Konvertiere die skalierten Features in einen DataFrame und sieh dir dessen Head an.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.DataFrame(scaled_features,columns=data.columns[:-1])\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "**Erstelle zwei Objekte X und y, die die skalierten Feature-Werte und Labels beinhalten.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nutze die `to_numpy()` Methode auf X und y und überschreibe sie mit dem neuen Ergebnis.**\n",
    "\n",
    "*Wir müssen dies tun, da Tensorflow mit Numpy Arrays arbeitet anstatt mit Pandas Series.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nutze SciKit Learn, um Trainings- und Testsets zu erzeugen.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrib.learn\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erstelle ein Objekt namens \"Classifier\", das ein `DNNClassifier` aus learn ist. Setze `classes` auf 2  und `hidden_units` auf [10,20,10].** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.contrib.learn.DNNClassifier(\n",
    "  feature_columns=feature_columns, hidden_units=[10, 20, 10], n_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jetzt fitte den classfier auf das Trainingsset. Nutze `steps=200` und `batch_size=20`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(x_train, y_train, steps=200, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell Auswertung\n",
    "\n",
    "**Nutze die `predict` Methode, um Vorhersagen für X_test zu erstellen.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_predictions = list(classifier.predict(x_test, as_iterable=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jetzt erstelle Callsification Report und Confusion Matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,note_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,note_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionaler Vergleich\n",
    "\n",
    "Dir sollten extrem akkurae Ergebnisse beim DNN Modell auffallen. Vergleichen wir das mit einem Random Forest Classifier für eine Einordnung der Ergebnisse.\n",
    "\n",
    "**Nutze SciKit Learn, um einen Random Forest Classifier zu erstellen. Vergleiche anschließend Classification Report und Confusion Matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_preds = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,rfc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,rfc_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es sollte auch ziemlich gut, aber nicht genauso gut wie das DNN Modell abgeschnitten haben. Das hat dir hoffentlich die Power der DNNs gezeigt.\n",
    "\n",
    "# Gut gemacht!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
